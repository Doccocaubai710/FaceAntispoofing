{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539aa4f3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:35.933471Z",
     "iopub.status.busy": "2024-06-23T12:20:35.933159Z",
     "iopub.status.idle": "2024-06-23T12:20:42.641498Z",
     "shell.execute_reply": "2024-06-23T12:20:42.640532Z"
    },
    "papermill": {
     "duration": 6.719545,
     "end_time": "2024-06-23T12:20:42.644345",
     "exception": false,
     "start_time": "2024-06-23T12:20:35.924800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os \n",
    "from torchvision import datasets,transforms,models,utils\n",
    "from torch.utils.data import DataLoader,ConcatDataset,Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re \n",
    "import timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Ignore warnings\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111617e",
   "metadata": {
    "papermill": {
     "duration": 0.008891,
     "end_time": "2024-06-23T12:20:42.663367",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.654476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CDCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c9287",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.681798Z",
     "iopub.status.busy": "2024-06-23T12:20:42.680870Z",
     "iopub.status.idle": "2024-06-23T12:20:42.696046Z",
     "shell.execute_reply": "2024-06-23T12:20:42.695143Z"
    },
    "papermill": {
     "duration": 0.026113,
     "end_time": "2024-06-23T12:20:42.698055",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.671942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FASDataset(Dataset):\n",
    "    def __init__(self,root_dir,depth_map_size,transform,smoothing):\n",
    "        super().__init__()\n",
    "        self.root_dir=root_dir\n",
    "        self.labels=self.assign_labels()\n",
    "        self.depth_map_size=depth_map_size\n",
    "        self.transform=transform\n",
    "        if smoothing:\n",
    "            self.label_weight=1.0\n",
    "        else:\n",
    "            self.label_weight=0.99\n",
    "    def assign_labels(self):\n",
    "        labels=[]\n",
    "        for parent_folder in os.listdir(self.root_dir):\n",
    "            parent_folder_path=os.path.join(self.root_dir,parent_folder)\n",
    "            if os.path.isdir(parent_folder_path):\n",
    "                label=1 if parent_folder=='real' else 0\n",
    "                image_files=os.listdir(parent_folder_path)\n",
    "                for image_file in image_files:\n",
    "                    image_path=os.path.join(parent_folder_path,image_file)\n",
    "                    labels.append((image_path,label))\n",
    "        return labels\n",
    "    def __getitem__(self,index):\n",
    "        image_path,label=self.labels[index]\n",
    "        img=Image.open(image_path)\n",
    "        if self.transform:\n",
    "            img=self.transform(img)\n",
    "        label=float(label)\n",
    "        label=np.expand_dims(label,axis=0)\n",
    "        if label==1:\n",
    "            depth_map=np.ones((self.depth_map_size[0],self.depth_map_size[1]),dtype=np.float32)*self.label_weight\n",
    "        else:\n",
    "            depth_map=np.ones((self.depth_map_size[0],self.depth_map_size[1]),dtype=np.float32)*(1.0-self.label_weight)\n",
    "        return img, depth_map,label\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0e014",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.713059Z",
     "iopub.status.busy": "2024-06-23T12:20:42.712725Z",
     "iopub.status.idle": "2024-06-23T12:20:42.717375Z",
     "shell.execute_reply": "2024-06-23T12:20:42.716473Z"
    },
    "papermill": {
     "duration": 0.014576,
     "end_time": "2024-06-23T12:20:42.719415",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.704839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "  _mean_=[0.5,0.5,0.5]\n",
    "  _sigma_= [0.5,0.5,0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0614",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.734735Z",
     "iopub.status.busy": "2024-06-23T12:20:42.733959Z",
     "iopub.status.idle": "2024-06-23T12:20:42.748270Z",
     "shell.execute_reply": "2024-06-23T12:20:42.747473Z"
    },
    "papermill": {
     "duration": 0.024135,
     "end_time": "2024-06-23T12:20:42.750361",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.726226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import ImageDraw\n",
    "\n",
    "def add_visualization(epoch,img_batch,preds,targets,score,writer):\n",
    "    mean=[-(_mean_[i])/(_sigma_[i]) for i in range(len(_mean_))]\n",
    "    sigma=[1/(_sigma_[i]) for i in range(len(_sigma_))]\n",
    "    img_transform=transforms.Compose([\n",
    "        transforms.Normalize(mean,sigma),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "    ts_transform=transforms.ToTensor()\n",
    "    for idx in range(img_batch.shape[0]):\n",
    "        vis_img = img_transform(img_batch[idx].cpu())\n",
    "        ImageDraw.Draw(vis_img).text((0,0), 'pred: {} vs gt: {}'.format(int(preds[idx]), int(targets[idx])), (255,0,255))\n",
    "        ImageDraw.Draw(vis_img).text((20,20), 'score {}'.format(score[idx]), (255,0,255))\n",
    "        tb_img = ts_transform(vis_img)\n",
    "        writer.add_image('Prediction visualization/{}'.format(idx), tb_img, epoch)\n",
    "\n",
    "def predict(depth_map,threshold=0.5):\n",
    "    \"\"\" Convert depth map estimation to true/fake prediction\n",
    "    Args:\n",
    "    -depth_map:32*32\n",
    "    -threshold: between 0 and 1\n",
    "    Return \n",
    "    -Predicted score\"\"\"\n",
    "    with torch.no_grad():\n",
    "        score=torch.mean(depth_map,axis=(1,2))\n",
    "        preds=(score>=threshold).type(torch.FloatTensor)\n",
    "        return preds,score\n",
    "def calc_accuracy(preds,targets):\n",
    "    with torch.no_grad():\n",
    "        equals=torch.mean(preds.eq(targets).type(torch.FloatTensor))\n",
    "        return equals.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98375943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.765253Z",
     "iopub.status.busy": "2024-06-23T12:20:42.764653Z",
     "iopub.status.idle": "2024-06-23T12:20:42.774630Z",
     "shell.execute_reply": "2024-06-23T12:20:42.773743Z"
    },
    "papermill": {
     "duration": 0.019571,
     "end_time": "2024-06-23T12:20:42.776653",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.757082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AvgMeter():\n",
    "    def __init__(self,writer,name,num_iter_per_epoch,per_iter_vis=False):\n",
    "        self.writer=writer\n",
    "        self.name=name\n",
    "        self.num_iter_per_epoch=num_iter_per_epoch\n",
    "        self.per_iter_vis=per_iter_vis\n",
    "    def reset(self,epoch):\n",
    "        self.val=0\n",
    "        self.avg=0\n",
    "        self.sum=0\n",
    "        self.count=0\n",
    "        self.epoch=epoch\n",
    "    def update(self,val,n=1):\n",
    "        self.val=val\n",
    "        self.sum+=val*n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count if self.count !=0 else 0\n",
    "        if self.per_iter_vis:\n",
    "            self.writer.add_scalar(self.name, self.avg, self.epoch * self.num_iter_per_epoch + self.count - 1)\n",
    "        else:\n",
    "            if self.count == self.num_iter_per_epoch - 1:\n",
    "                self.writer.add_scalar(self.name, self.avg, self.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa8d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.792704Z",
     "iopub.status.busy": "2024-06-23T12:20:42.791953Z",
     "iopub.status.idle": "2024-06-23T12:20:42.798492Z",
     "shell.execute_reply": "2024-06-23T12:20:42.797621Z"
    },
    "papermill": {
     "duration": 0.017128,
     "end_time": "2024-06-23T12:20:42.800983",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.783855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from random import random\n",
    "class RandomGammaCorrection:\n",
    "    def __init__(self,max_gamma,min_gamma):\n",
    "        self.max_gamma=max_gamma\n",
    "        self.min_gamma=min_gamma\n",
    "    def __call__(self,x):\n",
    "        gamma=self.min_gamma+random()*(self.max_gamma-self.min_gamma)\n",
    "        return TF.adjust_gamma(x,gamma=gamma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d0b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.822594Z",
     "iopub.status.busy": "2024-06-23T12:20:42.822256Z",
     "iopub.status.idle": "2024-06-23T12:20:42.852053Z",
     "shell.execute_reply": "2024-06-23T12:20:42.851210Z"
    },
    "papermill": {
     "duration": 0.043877,
     "end_time": "2024-06-23T12:20:42.854292",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.810415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################   Centeral-difference (second order, with 9 parameters and a const theta for 3x3 kernel) 2D Convolution   ##############################\n",
    "## | a1 a2 a3 |   | w1 w2 w3 |\n",
    "## | a4 a5 a6 | * | w4 w5 w6 | --> output = \\sum_{i=1}^{9}(ai * wi) - \\sum_{i=1}^{9}wi * a5 --> Conv2d (k=3) - Conv2d (k=1)\n",
    "## | a7 a8 a9 |   | w7 w8 w9 |\n",
    "##\n",
    "##   --> output = \n",
    "## | a1 a2 a3 |   |  w1  w2  w3 |     \n",
    "## | a4 a5 a6 | * |  w4  w5  w6 |  -  | a | * | w\\_sum |     (kernel_size=1x1, padding=0)\n",
    "## | a7 a8 a9 |   |  w7  w8  w9 |     \n",
    "\n",
    "class Conv2d_cd(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=3,stride=1,padding=1,dilation=1,groups=1,bias=False,theta=0.7):\n",
    "        super(Conv2d_cd,self).__init__()\n",
    "        self.conv=nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size,stride=stride,padding=padding,dilation=dilation,groups=groups,bias=bias)\n",
    "        self.theta=theta\n",
    "    def forward(self,x):\n",
    "        out_normal=self.conv(x)\n",
    "        if math.fabs(self.theta-0.0)<1e-8:\n",
    "            return out_normal\n",
    "        else:\n",
    "            #pdb.set_trace\n",
    "            [C_out,C_in,kernel_size,kernel_size]=self.conv.weight.shape\n",
    "            kernel_diff=self.conv.weight.sum(2).sum(2)\n",
    "            kernel_diff = kernel_diff[:, :, None, None]\n",
    "            out_diff = F.conv2d(input=x, weight=kernel_diff, bias=self.conv.bias, stride=self.conv.stride, padding=0, groups=self.conv.groups)\n",
    "            return out_normal-self.theta*out_diff\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self,kernel=3):\n",
    "        super(SpatialAttention,self).__init__()\n",
    "        self.conv1=nn.Conv2d(2,1,kernel_size=kernel,padding=kernel//2,bias=False)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        avg_out=torch.mean(x,dim=1,keepdim=True)\n",
    "        max_out,_=torch.max(x,dim=1,keepdim=True)\n",
    "        x=torch.cat([avg_out,max_out],dim=1)\n",
    "        x=self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "class CDCN(nn.Module):\n",
    "    def __init__(self,basic_conv=Conv2d_cd,theta=0.7):\n",
    "        super(CDCN,self).__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            basic_conv(3,64,kernel_size=3,stride=1,padding=1,bias=False,theta=theta),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        self.Block1=nn.Sequential(\n",
    "            basic_conv(64,128,kernel_size=3,stride=1,padding=1,bias=False,theta=theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            basic_conv(128, 196, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(196),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(196, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.Block2 = nn.Sequential(\n",
    "            basic_conv(128, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            basic_conv(128, 196, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(196),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(196, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),  \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.Block3 = nn.Sequential(\n",
    "            basic_conv(128, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            basic_conv(128, 196, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(196),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(196, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),   \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.lastconv1 = nn.Sequential(\n",
    "            basic_conv(128*3, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        self.lastconv2 = nn.Sequential(\n",
    "            basic_conv(128, 64, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        \n",
    "        self.lastconv3 = nn.Sequential(\n",
    "            basic_conv(64, 1, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        self.downsample32x32=nn.Upsample(size=(32,32),mode='bilinear')\n",
    "    def forward(self,x): # x=[3,256,256]\n",
    "        x_input=x\n",
    "        x=self.conv1(x)\n",
    "        x_Block1=self.Block1(x)\n",
    "        x_Block1_32x32=self.downsample32x32(x_Block1)\n",
    "        x_Block2 = self.Block2(x_Block1)\t    # x [128, 64, 64]\t  \n",
    "        x_Block2_32x32 = self.downsample32x32(x_Block2)   # x [128, 32, 32]  \n",
    "        \n",
    "        x_Block3 = self.Block3(x_Block2)\t    # x [128, 32, 32]  \t\n",
    "        x_Block3_32x32 = self.downsample32x32(x_Block3)   # x [128, 32, 32]  \n",
    "        x_concat = torch.cat((x_Block1_32x32,x_Block2_32x32,x_Block3_32x32), dim=1)    # x [128*3, 32, 32] \n",
    "        x=self.lastconv1(x_concat) #[128,32,32]\n",
    "        x=self.lastconv2(x)  #[64,32,32]\n",
    "        x=self.lastconv3(x)  #[1,32,32]\n",
    "        map_x=x.squeeze(1)\n",
    "        return map_x,x_concat,x_Block1,x_Block2,x_Block3,x_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675d55b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.869192Z",
     "iopub.status.busy": "2024-06-23T12:20:42.868909Z",
     "iopub.status.idle": "2024-06-23T12:20:42.890449Z",
     "shell.execute_reply": "2024-06-23T12:20:42.889589Z"
    },
    "papermill": {
     "duration": 0.031642,
     "end_time": "2024-06-23T12:20:42.892804",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.861162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CDCNpp(nn.Module):\n",
    "\n",
    "    def __init__(self, basic_conv=Conv2d_cd, theta=0.0):   \n",
    "        super(CDCNpp, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            basic_conv(3, 64, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),    \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.Block1 = nn.Sequential(\n",
    "            basic_conv(64, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),  \n",
    "            \n",
    "            basic_conv(128, int(128*1.6), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(int(128*1.6)),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(int(128*1.6), 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.Block2 = nn.Sequential(\n",
    "            basic_conv(128, int(128*1.2), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(int(128*1.2)),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(int(128*1.2), 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(128, int(128*1.4), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(int(128*1.4)),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(int(128*1.4), 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),  \n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.Block3 = nn.Sequential(\n",
    "            basic_conv(128, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            basic_conv(128, int(128*1.2), kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(int(128*1.2)),\n",
    "            nn.ReLU(),  \n",
    "            basic_conv(int(128*1.2), 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        \n",
    "        # Original\n",
    "        \n",
    "        self.lastconv1 = nn.Sequential(\n",
    "            basic_conv(128*3, 128, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            basic_conv(128, 1, kernel_size=3, stride=1, padding=1, bias=False, theta= theta),\n",
    "            nn.ReLU(),    \n",
    "        )\n",
    "        \n",
    "      \n",
    "        self.sa1 = SpatialAttention(kernel = 7)\n",
    "        self.sa2 = SpatialAttention(kernel = 5)\n",
    "        self.sa3 = SpatialAttention(kernel = 3)\n",
    "        self.downsample32x32 = nn.Upsample(size=(32, 32), mode='bilinear')\n",
    "\n",
    " \n",
    "    def forward(self, x):\t    \t# x [3, 256, 256]\n",
    "        \n",
    "        x_input = x\n",
    "        x = self.conv1(x)\t\t   \n",
    "        \n",
    "        x_Block1 = self.Block1(x)\t    \t    \t\n",
    "        attention1 = self.sa1(x_Block1)\n",
    "        x_Block1_SA = attention1 * x_Block1\n",
    "        x_Block1_32x32 = self.downsample32x32(x_Block1_SA)   \n",
    "        \n",
    "        x_Block2 = self.Block2(x_Block1)\t    \n",
    "        attention2 = self.sa2(x_Block2)  \n",
    "        x_Block2_SA = attention2 * x_Block2\n",
    "        x_Block2_32x32 = self.downsample32x32(x_Block2_SA)  \n",
    "        \n",
    "        x_Block3 = self.Block3(x_Block2)\t    \n",
    "        attention3 = self.sa3(x_Block3)  \n",
    "        x_Block3_SA = attention3 * x_Block3\t\n",
    "        x_Block3_32x32 = self.downsample32x32(x_Block3_SA)   \n",
    "        \n",
    "        x_concat = torch.cat((x_Block1_32x32,x_Block2_32x32,x_Block3_32x32), dim=1)    \n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        map_x = self.lastconv1(x_concat)\n",
    "        \n",
    "        map_x = map_x.squeeze(1)\n",
    "        \n",
    "        return map_x, x_concat, attention1, attention2, attention3, x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566053ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.911801Z",
     "iopub.status.busy": "2024-06-23T12:20:42.911528Z",
     "iopub.status.idle": "2024-06-23T12:20:42.927084Z",
     "shell.execute_reply": "2024-06-23T12:20:42.926214Z"
    },
    "papermill": {
     "duration": 0.02549,
     "end_time": "2024-06-23T12:20:42.929039",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.903549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contrast_depth_conv(input,device):\n",
    "    \"\"\"\n",
    "    compute contrast depth in both of (out,label)\n",
    "    input 32*32\n",
    "    output 8*32*32\n",
    "    \"\"\"\n",
    "    kernel_filter_list =[\n",
    "                        [[1,0,0],[0,-1,0],[0,0,0]], [[0,1,0],[0,-1,0],[0,0,0]], [[0,0,1],[0,-1,0],[0,0,0]],\n",
    "                        [[0,0,0],[1,-1,0],[0,0,0]], [[0,0,0],[0,-1,1],[0,0,0]],\n",
    "                        [[0,0,0],[0,-1,0],[1,0,0]], [[0,0,0],[0,-1,0],[0,1,0]], [[0,0,0],[0,-1,0],[0,0,1]]\n",
    "                        ]\n",
    "    kernel_filter=np.array(kernel_filter_list,np.float32)\n",
    "    kernel_filter=torch.from_numpy(kernel_filter.astype(np.float64)).float().to(device)\n",
    "    #weights (in_channel,out_channel,kernel,kernel)\n",
    "    kernel_filter=kernel_filter.unsqueeze(dim=1)\n",
    "    input = input.unsqueeze(dim=1).expand(input.shape[0], 8, input.shape[1],input.shape[2]) \n",
    "    contrast_depth = F.conv2d(input, weight=kernel_filter, groups=8)  # depthwise conv\n",
    "    return contrast_depth\n",
    "\n",
    "class ContrastDepthLoss(nn.Module): # Pearson range [-1, 1] so if < 0, abs|loss| ; if >0, 1- loss\n",
    "    def __init__(self,device):\n",
    "        super(ContrastDepthLoss,self).__init__()\n",
    "        self.device=device\n",
    "    def forward(self,out,label):\n",
    "        \"\"\"\n",
    "        compute contrast depth in both of (out,label),\n",
    "        \"\"\"\n",
    "        contrast_out=contrast_depth_conv(out,device=self.device)\n",
    "        contrast_label=contrast_depth_conv(label,device=self.device)\n",
    "        criterion_MSE=nn.MSELoss()\n",
    "        loss=criterion_MSE(contrast_out,contrast_label)\n",
    "        return loss\n",
    "class DepthLoss(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(DepthLoss,self).__init__()\n",
    "        self.criterion_absolute_loss=nn.MSELoss()\n",
    "        self.criterion_contrastive_loss=ContrastDepthLoss(device=device)\n",
    "    def forward(self,predicted_depth_map,gt_depth_map):\n",
    "        absolute_loss=self.criterion_absolute_loss(predicted_depth_map,gt_depth_map)\n",
    "        contrastive_loss=self.criterion_contrastive_loss(predicted_depth_map,gt_depth_map)\n",
    "        return absolute_loss+contrastive_loss\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e9cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:42.943766Z",
     "iopub.status.busy": "2024-06-23T12:20:42.943426Z",
     "iopub.status.idle": "2024-06-23T12:20:54.498607Z",
     "shell.execute_reply": "2024-06-23T12:20:54.497558Z"
    },
    "papermill": {
     "duration": 11.565037,
     "end_time": "2024-06-23T12:20:54.500839",
     "exception": false,
     "start_time": "2024-06-23T12:20:42.935802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from random import randint\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=CDCNpp().to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.0003)\n",
    "lr_scheduler=StepLR(optimizer=optimizer,step_size=30,gamma=0.1)\n",
    "criterion=DepthLoss(device=device)\n",
    "writer=SummaryWriter()\n",
    "dump_input=torch.randn((1,3,256,256)).to(device)\n",
    "writer.add_graph(model,dump_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680223e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:54.516168Z",
     "iopub.status.busy": "2024-06-23T12:20:54.515680Z",
     "iopub.status.idle": "2024-06-23T12:20:54.564096Z",
     "shell.execute_reply": "2024-06-23T12:20:54.563221Z"
    },
    "papermill": {
     "duration": 0.05807,
     "end_time": "2024-06-23T12:20:54.565991",
     "exception": false,
     "start_time": "2024-06-23T12:20:54.507921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import math\n",
    "dump_input=torch.randn((1,3,256,256)).to(device)\n",
    "output=model(dump_input)\n",
    "output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3cba8",
   "metadata": {
    "papermill": {
     "duration": 0.006749,
     "end_time": "2024-06-23T12:20:54.579714",
     "exception": false,
     "start_time": "2024-06-23T12:20:54.572965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54cc2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:54.594825Z",
     "iopub.status.busy": "2024-06-23T12:20:54.594162Z",
     "iopub.status.idle": "2024-06-23T12:20:54.604068Z",
     "shell.execute_reply": "2024-06-23T12:20:54.603382Z"
    },
    "papermill": {
     "duration": 0.019431,
     "end_time": "2024-06-23T12:20:54.605911",
     "exception": false,
     "start_time": "2024-06-23T12:20:54.586480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_flipped = transforms.Compose([\n",
    "    RandomGammaCorrection(max_gamma=1.5,min_gamma=0.67),\n",
    "    transforms.Resize([256,256]),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "spoof_transforms = transforms.Compose([\n",
    "    RandomGammaCorrection(max_gamma=1.5,min_gamma=0.67),\n",
    "    transforms.Resize([256,256]),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "    transforms.RandomHorizontalFlip(p=1),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "normal_transforms=transforms.Compose([\n",
    "    transforms.Resize([256,256]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "    \n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649efb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:54.620580Z",
     "iopub.status.busy": "2024-06-23T12:20:54.620122Z",
     "iopub.status.idle": "2024-06-23T12:20:55.217495Z",
     "shell.execute_reply": "2024-06-23T12:20:55.216767Z"
    },
    "papermill": {
     "duration": 0.60696,
     "end_time": "2024-06-23T12:20:55.219638",
     "exception": false,
     "start_time": "2024-06-23T12:20:54.612678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_orig=FASDataset(root_dir=\"/kaggle/input/hehedataset/Combine/train\",\n",
    "                    depth_map_size=[32,32],\n",
    "                    transform=transform_flipped,\n",
    "                    smoothing=True)\n",
    "train_flip = FASDataset(root_dir=\"/kaggle/input/hehedataset/Combine/train\",\n",
    "                           depth_map_size=[32,32],\n",
    "                           transform=spoof_transforms,\n",
    "                           smoothing=True)\n",
    "train_data_combined = ConcatDataset([train_orig, train_flip])\n",
    "train_loader=torch.utils.data.DataLoader(\n",
    "                dataset=train_data_combined,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8aa03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:55.234616Z",
     "iopub.status.busy": "2024-06-23T12:20:55.234350Z",
     "iopub.status.idle": "2024-06-23T12:20:56.075686Z",
     "shell.execute_reply": "2024-06-23T12:20:56.074922Z"
    },
    "papermill": {
     "duration": 0.851213,
     "end_time": "2024-06-23T12:20:56.077941",
     "exception": false,
     "start_time": "2024-06-23T12:20:55.226728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_orig=FASDataset(root_dir=\"/kaggle/input/hehedataset/Combine/val\",\n",
    "                   \n",
    "                    depth_map_size=[32,32],\n",
    "                    transform=transform_flipped,\n",
    "                    smoothing=True)\n",
    "val_flip = FASDataset(root_dir=\"/kaggle/input/hehedataset/Combine/val\",\n",
    "                           depth_map_size=[32,32],\n",
    "                           transform=spoof_transforms,\n",
    "                           smoothing=True)\n",
    "val_data_combined = ConcatDataset([val_orig, val_flip])\n",
    "val_loader=torch.utils.data.DataLoader(\n",
    "                dataset=val_data_combined,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                num_workers=2)\n",
    "testset = FASDataset(root_dir=\"/kaggle/input/hehedataset/Combine/test\",\n",
    "                depth_map_size=[32, 32],\n",
    "                transform=normal_transforms,\n",
    "                smoothing=False)\n",
    "test_loader=torch.utils.data.DataLoader(\n",
    "                dataset=testset,\n",
    "                batch_size=8,\n",
    "                shuffle=True,\n",
    "                num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065f02c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:56.092945Z",
     "iopub.status.busy": "2024-06-23T12:20:56.092678Z",
     "iopub.status.idle": "2024-06-23T12:20:56.097365Z",
     "shell.execute_reply": "2024-06-23T12:20:56.096572Z"
    },
    "papermill": {
     "duration": 0.014154,
     "end_time": "2024-06-23T12:20:56.099208",
     "exception": false,
     "start_time": "2024-06-23T12:20:56.085054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(train_data_combined),len(val_data_combined),len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de668ee6",
   "metadata": {
    "papermill": {
     "duration": 0.006848,
     "end_time": "2024-06-23T12:20:56.112847",
     "exception": false,
     "start_time": "2024-06-23T12:20:56.105999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7e00b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:56.128100Z",
     "iopub.status.busy": "2024-06-23T12:20:56.127856Z",
     "iopub.status.idle": "2024-06-23T12:20:56.142339Z",
     "shell.execute_reply": "2024-06-23T12:20:56.141524Z"
    },
    "papermill": {
     "duration": 0.024257,
     "end_time": "2024-06-23T12:20:56.144212",
     "exception": false,
     "start_time": "2024-06-23T12:20:56.119955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "train_loss_metric=AvgMeter(writer=writer, name=\"Loss/train\", num_iter_per_epoch=len(train_loader), per_iter_vis=True)\n",
    "train_acc_metric = AvgMeter(writer=writer, name='Accuracy/train', num_iter_per_epoch=len(train_loader), per_iter_vis=True)\n",
    "val_loss_metric = AvgMeter(writer=writer, name='Loss/val', num_iter_per_epoch=len(val_loader))\n",
    "val_acc_metric = AvgMeter(writer=writer, name='Accuracy/val', num_iter_per_epoch=len(val_loader))\n",
    "\n",
    "\n",
    "def train_one_epoch(model,epoch, train_loader, device, optimizer, criterion, scheduler_lr=None):\n",
    "    model.train()\n",
    "    train_loss_metric.reset(epoch)\n",
    "    train_acc_metric.reset(epoch)\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}\", position=0, leave=True)\n",
    "    for i, (img, depth_map, label) in pbar:\n",
    "        img, depth_map, label = img.to(device), depth_map.to(device), label.to(device)\n",
    "        net_depth_map, _, _, _, _, _ = model(img)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(net_depth_map, depth_map)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds, _ = predict(net_depth_map)\n",
    "        targets, _ = predict(depth_map)\n",
    "        accuracy = calc_accuracy(preds, targets)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss_metric.update(loss.item())\n",
    "        train_acc_metric.update(accuracy)\n",
    "\n",
    "        pbar.set_postfix({'Loss': train_loss_metric.avg, 'Accuracy': train_acc_metric.avg})\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def validate_one_epoch(model,epoch, val_loader, device, criterion, scheduler_lr=None):\n",
    "    model.eval()\n",
    "    val_loss_metric.reset(epoch)\n",
    "    val_acc_metric.reset(epoch)\n",
    "    seed = randint(0, len(val_loader) - 1)\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Validation Epoch {epoch}\", position=0, leave=True)\n",
    "        for i, (img, depth_map, label) in pbar:\n",
    "            img, depth_map, label = img.to(device), depth_map.to(device), label.to(device)\n",
    "            net_depth_map, _, _, _, _, _ = model(img)\n",
    "            loss = criterion(net_depth_map, depth_map)\n",
    "            preds, score = predict(net_depth_map)\n",
    "            targets, _ = predict(depth_map)\n",
    "            accuracy = calc_accuracy(preds, targets)\n",
    "\n",
    "            # Update metrics\n",
    "            val_loss_metric.update(loss.item())\n",
    "            val_acc_metric.update(accuracy)\n",
    "\n",
    "            pbar.set_postfix({'Loss': val_loss_metric.avg, 'Accuracy': val_acc_metric.avg})\n",
    "\n",
    "            if i == seed:\n",
    "                add_visualization(epoch, img, preds, targets, score, writer)\n",
    "    return val_acc_metric.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ffbb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:56.158928Z",
     "iopub.status.busy": "2024-06-23T12:20:56.158690Z",
     "iopub.status.idle": "2024-06-23T12:20:56.165202Z",
     "shell.execute_reply": "2024-06-23T12:20:56.164433Z"
    },
    "papermill": {
     "duration": 0.015914,
     "end_time": "2024-06-23T12:20:56.166970",
     "exception": false,
     "start_time": "2024-06-23T12:20:56.151056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluating(model, test_loader, device, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for img, depth_map, label in test_loader:\n",
    "            img, depth_map, label = img.to(device), depth_map.to(device), label.to(device)\n",
    "            net_depth_map, _, _, _, _, _ = model(img)\n",
    "            loss = criterion(net_depth_map, depth_map)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            preds, _ = predict(net_depth_map)\n",
    "            targets, _ = predict(depth_map)\n",
    "            correct += preds.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = correct / total\n",
    "    \n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36da83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T12:20:56.181940Z",
     "iopub.status.busy": "2024-06-23T12:20:56.181695Z",
     "iopub.status.idle": "2024-06-23T21:40:48.783609Z",
     "shell.execute_reply": "2024-06-23T21:40:48.782470Z"
    },
    "papermill": {
     "duration": 33603.19263,
     "end_time": "2024-06-23T21:40:59.366596",
     "exception": false,
     "start_time": "2024-06-23T12:20:56.173966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_acc=0.0\n",
    "for epoch in range(0, 35):\n",
    "    train_one_epoch(model, epoch, train_loader, device, optimizer, criterion)\n",
    "    val_acc = validate_one_epoch(model, epoch, val_loader, device, criterion)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_acc, test_loss = evaluating(model, test_loader, device, criterion)\n",
    "    \n",
    "    # Print test loss and accuracy\n",
    "    print(f\"Epoch {epoch} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Check if this is the best accuracy so far\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"New best model saved with accuracy: {best_acc:.4f}\")\n",
    "\n",
    "    torch.save(model,'model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b836bb",
   "metadata": {
    "papermill": {
     "duration": 10.325515,
     "end_time": "2024-06-23T21:41:20.320320",
     "exception": false,
     "start_time": "2024-06-23T21:41:09.994805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f874ddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-23T21:41:41.257332Z",
     "iopub.status.busy": "2024-06-23T21:41:41.256521Z",
     "iopub.status.idle": "2024-06-23T21:42:26.005978Z",
     "shell.execute_reply": "2024-06-23T21:42:26.004855Z"
    },
    "papermill": {
     "duration": 55.305767,
     "end_time": "2024-06-23T21:42:26.008178",
     "exception": false,
     "start_time": "2024-06-23T21:41:30.702411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    total = 0  # Total number of samples\n",
    "\n",
    "    for i, (img, depth_map, label) in enumerate(test_loader):\n",
    "        img, depth_map, label = img.to(device), depth_map.to(device), label.to(device)\n",
    "        net_depth_map, _, _, _, _, _ = model(img)\n",
    "        preds, score = predict(net_depth_map)\n",
    "        targets, target_score = predict(depth_map)\n",
    "\n",
    "        tp += (preds.eq(1) & targets.eq(1)).sum().item()\n",
    "        tn += (preds.eq(0) & targets.eq(0)).sum().item()\n",
    "        fp += (preds.eq(1) & targets.eq(0)).sum().item()\n",
    "        fn += (preds.eq(0) & targets.eq(1)).sum().item()\n",
    "\n",
    "        correct += preds.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    far = fp / (fp + tn)\n",
    "    frr = fn / (fn + tp)\n",
    "    recall = tp / (tp + fn)\n",
    "    hter = (far + frr) / 2\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"FAR: {far * 100:.2f}%\")\n",
    "    print(f\"FRR: {frr * 100:.2f}%\")\n",
    "    print(f\"HTER: {hter * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5194945,
     "sourceId": 8668638,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5197405,
     "sourceId": 8672053,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5266247,
     "sourceId": 8764568,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33726.233969,
   "end_time": "2024-06-23T21:42:39.545342",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-23T12:20:33.311373",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
